{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilytsen/multi_layer_perceptor/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP - MULTI LAYER PERCEPTOR"
      ],
      "metadata": {
        "id": "O4BCje_05Uxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4DXI4oN4RoV"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn import metrics, datasets\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#carregar conjunto de dados\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "da1KX3TS4o1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separar conjunto de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "KYIt5R3w5DbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar algoritmo de Rede Neural Multi-Layer Perceptron (MLP) para classificação\n",
        "clf = MLPClassifier(\n",
        "    solver='sgd',              # Define o otimizador como 'Stochastic Gradient Descent' (SGD), que ajusta os pesos usando gradiente descendente estocástico.\n",
        "    alpha=0.05,                # Coeficiente de regularização L2 (também conhecido como penalização de peso) para evitar overfitting. Controla a magnitude da regularização.\n",
        "    activation='relu',         # Função de ativação usada nas camadas ocultas. 'relu' (Rectified Linear Unit) é uma função não-linear que retorna zero para valores negativos e o valor de entrada para valores positivos.\n",
        "    hidden_layer_sizes=(3),    # Define uma camada oculta com 3 neurônios. Pode especificar várias camadas ocultas como uma tupla, por exemplo, (3, 5) criaria duas camadas ocultas, uma com 3 e outra com 5 neurônios.\n",
        "    max_iter=1000,             # Número máximo de iterações (épocas) para o treinamento da rede neural.\n",
        "    verbose=True               # Exibe informações detalhadas durante o treinamento, útil para acompanhar o progresso e depuração.\n",
        ")"
      ],
      "metadata": {
        "id": "huw6n5N85M5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinando o modelo\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SCkDIGdw6tzK",
        "outputId": "fbb8efb6-010a-4054-d858-4372171415a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.97761318\n",
            "Iteration 2, loss = 1.91506523\n",
            "Iteration 3, loss = 1.82831523\n",
            "Iteration 4, loss = 1.72135066\n",
            "Iteration 5, loss = 1.59859850\n",
            "Iteration 6, loss = 1.46656634\n",
            "Iteration 7, loss = 1.33031081\n",
            "Iteration 8, loss = 1.19627381\n",
            "Iteration 9, loss = 1.07077995\n",
            "Iteration 10, loss = 0.95906427\n",
            "Iteration 11, loss = 0.86520709\n",
            "Iteration 12, loss = 0.79151690\n",
            "Iteration 13, loss = 0.73811212\n",
            "Iteration 14, loss = 0.70297371\n",
            "Iteration 15, loss = 0.68253101\n",
            "Iteration 16, loss = 0.67254282\n",
            "Iteration 17, loss = 0.66891575\n",
            "Iteration 18, loss = 0.66823121\n",
            "Iteration 19, loss = 0.66795586\n",
            "Iteration 20, loss = 0.66642890\n",
            "Iteration 21, loss = 0.66289648\n",
            "Iteration 22, loss = 0.65726929\n",
            "Iteration 23, loss = 0.64995058\n",
            "Iteration 24, loss = 0.64131909\n",
            "Iteration 25, loss = 0.63255190\n",
            "Iteration 26, loss = 0.62420331\n",
            "Iteration 27, loss = 0.61616188\n",
            "Iteration 28, loss = 0.60857080\n",
            "Iteration 29, loss = 0.60147800\n",
            "Iteration 30, loss = 0.59511955\n",
            "Iteration 31, loss = 0.58955934\n",
            "Iteration 32, loss = 0.58466772\n",
            "Iteration 33, loss = 0.58039575\n",
            "Iteration 34, loss = 0.57659758\n",
            "Iteration 35, loss = 0.57318265\n",
            "Iteration 36, loss = 0.57006928\n",
            "Iteration 37, loss = 0.56718968\n",
            "Iteration 38, loss = 0.56446107\n",
            "Iteration 39, loss = 0.56185988\n",
            "Iteration 40, loss = 0.55933883\n",
            "Iteration 41, loss = 0.55687289\n",
            "Iteration 42, loss = 0.55437886\n",
            "Iteration 43, loss = 0.55189163\n",
            "Iteration 44, loss = 0.54941628\n",
            "Iteration 45, loss = 0.54703766\n",
            "Iteration 46, loss = 0.54471890\n",
            "Iteration 47, loss = 0.54247286\n",
            "Iteration 48, loss = 0.54034948\n",
            "Iteration 49, loss = 0.53836894\n",
            "Iteration 50, loss = 0.53651102\n",
            "Iteration 51, loss = 0.53486596\n",
            "Iteration 52, loss = 0.53333750\n",
            "Iteration 53, loss = 0.53190436\n",
            "Iteration 54, loss = 0.53052990\n",
            "Iteration 55, loss = 0.52919504\n",
            "Iteration 56, loss = 0.52788849\n",
            "Iteration 57, loss = 0.52659874\n",
            "Iteration 58, loss = 0.52531891\n",
            "Iteration 59, loss = 0.52405445\n",
            "Iteration 60, loss = 0.52279723\n",
            "Iteration 61, loss = 0.52155456\n",
            "Iteration 62, loss = 0.52034698\n",
            "Iteration 63, loss = 0.51916374\n",
            "Iteration 64, loss = 0.51799995\n",
            "Iteration 65, loss = 0.51684511\n",
            "Iteration 66, loss = 0.51569677\n",
            "Iteration 67, loss = 0.51455720\n",
            "Iteration 68, loss = 0.51342874\n",
            "Iteration 69, loss = 0.51231686\n",
            "Iteration 70, loss = 0.51122577\n",
            "Iteration 71, loss = 0.51014966\n",
            "Iteration 72, loss = 0.50908887\n",
            "Iteration 73, loss = 0.50804321\n",
            "Iteration 74, loss = 0.50701203\n",
            "Iteration 75, loss = 0.50599437\n",
            "Iteration 76, loss = 0.50498908\n",
            "Iteration 77, loss = 0.50399493\n",
            "Iteration 78, loss = 0.50301074\n",
            "Iteration 79, loss = 0.50203544\n",
            "Iteration 80, loss = 0.50106815\n",
            "Iteration 81, loss = 0.50010817\n",
            "Iteration 82, loss = 0.49915501\n",
            "Iteration 83, loss = 0.49820837\n",
            "Iteration 84, loss = 0.49726810\n",
            "Iteration 85, loss = 0.49633417\n",
            "Iteration 86, loss = 0.49540660\n",
            "Iteration 87, loss = 0.49448549\n",
            "Iteration 88, loss = 0.49357092\n",
            "Iteration 89, loss = 0.49266295\n",
            "Iteration 90, loss = 0.49176163\n",
            "Iteration 91, loss = 0.49086694\n",
            "Iteration 92, loss = 0.48997882\n",
            "Iteration 93, loss = 0.48909719\n",
            "Iteration 94, loss = 0.48822191\n",
            "Iteration 95, loss = 0.48735283\n",
            "Iteration 96, loss = 0.48648979\n",
            "Iteration 97, loss = 0.48563261\n",
            "Iteration 98, loss = 0.48478113\n",
            "Iteration 99, loss = 0.48393519\n",
            "Iteration 100, loss = 0.48309466\n",
            "Iteration 101, loss = 0.48225942\n",
            "Iteration 102, loss = 0.48142934\n",
            "Iteration 103, loss = 0.48060435\n",
            "Iteration 104, loss = 0.47978634\n",
            "Iteration 105, loss = 0.47897438\n",
            "Iteration 106, loss = 0.47816732\n",
            "Iteration 107, loss = 0.47736511\n",
            "Iteration 108, loss = 0.47656773\n",
            "Iteration 109, loss = 0.47577513\n",
            "Iteration 110, loss = 0.47498727\n",
            "Iteration 111, loss = 0.47420411\n",
            "Iteration 112, loss = 0.47342560\n",
            "Iteration 113, loss = 0.47265168\n",
            "Iteration 114, loss = 0.47188230\n",
            "Iteration 115, loss = 0.47111739\n",
            "Iteration 116, loss = 0.47035690\n",
            "Iteration 117, loss = 0.46960076\n",
            "Iteration 118, loss = 0.46884891\n",
            "Iteration 119, loss = 0.46810127\n",
            "Iteration 120, loss = 0.46735780\n",
            "Iteration 121, loss = 0.46661842\n",
            "Iteration 122, loss = 0.46588309\n",
            "Iteration 123, loss = 0.46515173\n",
            "Iteration 124, loss = 0.46442430\n",
            "Iteration 125, loss = 0.46370073\n",
            "Iteration 126, loss = 0.46298096\n",
            "Iteration 127, loss = 0.46226496\n",
            "Iteration 128, loss = 0.46155265\n",
            "Iteration 129, loss = 0.46084399\n",
            "Iteration 130, loss = 0.46013892\n",
            "Iteration 131, loss = 0.45943739\n",
            "Iteration 132, loss = 0.45873935\n",
            "Iteration 133, loss = 0.45804475\n",
            "Iteration 134, loss = 0.45735531\n",
            "Iteration 135, loss = 0.45666971\n",
            "Iteration 136, loss = 0.45598751\n",
            "Iteration 137, loss = 0.45530868\n",
            "Iteration 138, loss = 0.45463320\n",
            "Iteration 139, loss = 0.45396104\n",
            "Iteration 140, loss = 0.45329215\n",
            "Iteration 141, loss = 0.45262650\n",
            "Iteration 142, loss = 0.45196406\n",
            "Iteration 143, loss = 0.45130479\n",
            "Iteration 144, loss = 0.45064865\n",
            "Iteration 145, loss = 0.44999560\n",
            "Iteration 146, loss = 0.44934559\n",
            "Iteration 147, loss = 0.44869977\n",
            "Iteration 148, loss = 0.44805724\n",
            "Iteration 149, loss = 0.44741765\n",
            "Iteration 150, loss = 0.44678100\n",
            "Iteration 151, loss = 0.44614730\n",
            "Iteration 152, loss = 0.44551654\n",
            "Iteration 153, loss = 0.44488943\n",
            "Iteration 154, loss = 0.44426512\n",
            "Iteration 155, loss = 0.44364355\n",
            "Iteration 156, loss = 0.44302528\n",
            "Iteration 157, loss = 0.44241004\n",
            "Iteration 158, loss = 0.44179764\n",
            "Iteration 159, loss = 0.44118807\n",
            "Iteration 160, loss = 0.44058127\n",
            "Iteration 161, loss = 0.43997718\n",
            "Iteration 162, loss = 0.43937603\n",
            "Iteration 163, loss = 0.43877773\n",
            "Iteration 164, loss = 0.43818239\n",
            "Iteration 165, loss = 0.43758980\n",
            "Iteration 166, loss = 0.43699980\n",
            "Iteration 167, loss = 0.43641293\n",
            "Iteration 168, loss = 0.43582846\n",
            "Iteration 169, loss = 0.43524658\n",
            "Iteration 170, loss = 0.43466721\n",
            "Iteration 171, loss = 0.43409083\n",
            "Iteration 172, loss = 0.43351687\n",
            "Iteration 173, loss = 0.43294592\n",
            "Iteration 174, loss = 0.43237738\n",
            "Iteration 175, loss = 0.43181144\n",
            "Iteration 176, loss = 0.43124857\n",
            "Iteration 177, loss = 0.43068825\n",
            "Iteration 178, loss = 0.43013075\n",
            "Iteration 179, loss = 0.42957600\n",
            "Iteration 180, loss = 0.42902433\n",
            "Iteration 181, loss = 0.42847487\n",
            "Iteration 182, loss = 0.42792769\n",
            "Iteration 183, loss = 0.42738301\n",
            "Iteration 184, loss = 0.42684079\n",
            "Iteration 185, loss = 0.42630067\n",
            "Iteration 186, loss = 0.42576253\n",
            "Iteration 187, loss = 0.42522633\n",
            "Iteration 188, loss = 0.42469201\n",
            "Iteration 189, loss = 0.42415987\n",
            "Iteration 190, loss = 0.42363012\n",
            "Iteration 191, loss = 0.42310230\n",
            "Iteration 192, loss = 0.42257703\n",
            "Iteration 193, loss = 0.42205454\n",
            "Iteration 194, loss = 0.42153397\n",
            "Iteration 195, loss = 0.42101513\n",
            "Iteration 196, loss = 0.42049796\n",
            "Iteration 197, loss = 0.41998242\n",
            "Iteration 198, loss = 0.41946845\n",
            "Iteration 199, loss = 0.41895613\n",
            "Iteration 200, loss = 0.41844573\n",
            "Iteration 201, loss = 0.41793682\n",
            "Iteration 202, loss = 0.41742948\n",
            "Iteration 203, loss = 0.41692375\n",
            "Iteration 204, loss = 0.41641942\n",
            "Iteration 205, loss = 0.41591647\n",
            "Iteration 206, loss = 0.41541486\n",
            "Iteration 207, loss = 0.41491456\n",
            "Iteration 208, loss = 0.41441555\n",
            "Iteration 209, loss = 0.41391815\n",
            "Iteration 210, loss = 0.41342267\n",
            "Iteration 211, loss = 0.41292853\n",
            "Iteration 212, loss = 0.41243564\n",
            "Iteration 213, loss = 0.41194396\n",
            "Iteration 214, loss = 0.41145347\n",
            "Iteration 215, loss = 0.41096415\n",
            "Iteration 216, loss = 0.41047596\n",
            "Iteration 217, loss = 0.40998890\n",
            "Iteration 218, loss = 0.40950320\n",
            "Iteration 219, loss = 0.40901871\n",
            "Iteration 220, loss = 0.40853530\n",
            "Iteration 221, loss = 0.40805295\n",
            "Iteration 222, loss = 0.40757163\n",
            "Iteration 223, loss = 0.40709133\n",
            "Iteration 224, loss = 0.40661203\n",
            "Iteration 225, loss = 0.40613371\n",
            "Iteration 226, loss = 0.40565636\n",
            "Iteration 227, loss = 0.40518025\n",
            "Iteration 228, loss = 0.40470515\n",
            "Iteration 229, loss = 0.40423142\n",
            "Iteration 230, loss = 0.40375864\n",
            "Iteration 231, loss = 0.40328679\n",
            "Iteration 232, loss = 0.40281585\n",
            "Iteration 233, loss = 0.40234580\n",
            "Iteration 234, loss = 0.40187663\n",
            "Iteration 235, loss = 0.40140832\n",
            "Iteration 236, loss = 0.40094108\n",
            "Iteration 237, loss = 0.40047486\n",
            "Iteration 238, loss = 0.40001004\n",
            "Iteration 239, loss = 0.39954735\n",
            "Iteration 240, loss = 0.39908557\n",
            "Iteration 241, loss = 0.39862475\n",
            "Iteration 242, loss = 0.39816474\n",
            "Iteration 243, loss = 0.39770554\n",
            "Iteration 244, loss = 0.39724713\n",
            "Iteration 245, loss = 0.39678951\n",
            "Iteration 246, loss = 0.39633266\n",
            "Iteration 247, loss = 0.39587657\n",
            "Iteration 248, loss = 0.39542123\n",
            "Iteration 249, loss = 0.39496664\n",
            "Iteration 250, loss = 0.39451278\n",
            "Iteration 251, loss = 0.39405964\n",
            "Iteration 252, loss = 0.39360722\n",
            "Iteration 253, loss = 0.39315550\n",
            "Iteration 254, loss = 0.39270447\n",
            "Iteration 255, loss = 0.39225412\n",
            "Iteration 256, loss = 0.39180445\n",
            "Iteration 257, loss = 0.39135545\n",
            "Iteration 258, loss = 0.39090710\n",
            "Iteration 259, loss = 0.39045939\n",
            "Iteration 260, loss = 0.39001233\n",
            "Iteration 261, loss = 0.38956589\n",
            "Iteration 262, loss = 0.38912007\n",
            "Iteration 263, loss = 0.38867486\n",
            "Iteration 264, loss = 0.38823025\n",
            "Iteration 265, loss = 0.38778624\n",
            "Iteration 266, loss = 0.38734282\n",
            "Iteration 267, loss = 0.38689997\n",
            "Iteration 268, loss = 0.38645770\n",
            "Iteration 269, loss = 0.38601599\n",
            "Iteration 270, loss = 0.38557484\n",
            "Iteration 271, loss = 0.38513425\n",
            "Iteration 272, loss = 0.38469419\n",
            "Iteration 273, loss = 0.38425468\n",
            "Iteration 274, loss = 0.38381571\n",
            "Iteration 275, loss = 0.38337726\n",
            "Iteration 276, loss = 0.38293933\n",
            "Iteration 277, loss = 0.38250220\n",
            "Iteration 278, loss = 0.38206564\n",
            "Iteration 279, loss = 0.38162961\n",
            "Iteration 280, loss = 0.38119410\n",
            "Iteration 281, loss = 0.38075909\n",
            "Iteration 282, loss = 0.38032459\n",
            "Iteration 283, loss = 0.37989062\n",
            "Iteration 284, loss = 0.37945746\n",
            "Iteration 285, loss = 0.37902480\n",
            "Iteration 286, loss = 0.37859263\n",
            "Iteration 287, loss = 0.37816094\n",
            "Iteration 288, loss = 0.37772974\n",
            "Iteration 289, loss = 0.37729900\n",
            "Iteration 290, loss = 0.37686873\n",
            "Iteration 291, loss = 0.37643939\n",
            "Iteration 292, loss = 0.37601077\n",
            "Iteration 293, loss = 0.37558258\n",
            "Iteration 294, loss = 0.37515482\n",
            "Iteration 295, loss = 0.37472751\n",
            "Iteration 296, loss = 0.37430064\n",
            "Iteration 297, loss = 0.37387421\n",
            "Iteration 298, loss = 0.37344822\n",
            "Iteration 299, loss = 0.37302268\n",
            "Iteration 300, loss = 0.37259758\n",
            "Iteration 301, loss = 0.37217292\n",
            "Iteration 302, loss = 0.37174871\n",
            "Iteration 303, loss = 0.37132493\n",
            "Iteration 304, loss = 0.37090160\n",
            "Iteration 305, loss = 0.37047870\n",
            "Iteration 306, loss = 0.37005624\n",
            "Iteration 307, loss = 0.36963421\n",
            "Iteration 308, loss = 0.36921261\n",
            "Iteration 309, loss = 0.36879143\n",
            "Iteration 310, loss = 0.36837068\n",
            "Iteration 311, loss = 0.36795034\n",
            "Iteration 312, loss = 0.36753042\n",
            "Iteration 313, loss = 0.36711092\n",
            "Iteration 314, loss = 0.36669182\n",
            "Iteration 315, loss = 0.36627313\n",
            "Iteration 316, loss = 0.36585484\n",
            "Iteration 317, loss = 0.36543694\n",
            "Iteration 318, loss = 0.36501945\n",
            "Iteration 319, loss = 0.36460234\n",
            "Iteration 320, loss = 0.36418562\n",
            "Iteration 321, loss = 0.36376929\n",
            "Iteration 322, loss = 0.36335334\n",
            "Iteration 323, loss = 0.36293777\n",
            "Iteration 324, loss = 0.36252258\n",
            "Iteration 325, loss = 0.36210776\n",
            "Iteration 326, loss = 0.36169332\n",
            "Iteration 327, loss = 0.36127924\n",
            "Iteration 328, loss = 0.36086554\n",
            "Iteration 329, loss = 0.36045220\n",
            "Iteration 330, loss = 0.36003922\n",
            "Iteration 331, loss = 0.35962661\n",
            "Iteration 332, loss = 0.35921435\n",
            "Iteration 333, loss = 0.35880245\n",
            "Iteration 334, loss = 0.35839091\n",
            "Iteration 335, loss = 0.35797973\n",
            "Iteration 336, loss = 0.35756890\n",
            "Iteration 337, loss = 0.35715842\n",
            "Iteration 338, loss = 0.35674830\n",
            "Iteration 339, loss = 0.35633888\n",
            "Iteration 340, loss = 0.35592978\n",
            "Iteration 341, loss = 0.35552101\n",
            "Iteration 342, loss = 0.35511257\n",
            "Iteration 343, loss = 0.35470447\n",
            "Iteration 344, loss = 0.35429671\n",
            "Iteration 345, loss = 0.35388931\n",
            "Iteration 346, loss = 0.35348226\n",
            "Iteration 347, loss = 0.35307558\n",
            "Iteration 348, loss = 0.35266932\n",
            "Iteration 349, loss = 0.35226339\n",
            "Iteration 350, loss = 0.35185783\n",
            "Iteration 351, loss = 0.35145263\n",
            "Iteration 352, loss = 0.35104784\n",
            "Iteration 353, loss = 0.35064334\n",
            "Iteration 354, loss = 0.35023922\n",
            "Iteration 355, loss = 0.34983545\n",
            "Iteration 356, loss = 0.34943206\n",
            "Iteration 357, loss = 0.34902901\n",
            "Iteration 358, loss = 0.34862632\n",
            "Iteration 359, loss = 0.34822397\n",
            "Iteration 360, loss = 0.34782198\n",
            "Iteration 361, loss = 0.34742034\n",
            "Iteration 362, loss = 0.34701904\n",
            "Iteration 363, loss = 0.34661811\n",
            "Iteration 364, loss = 0.34621752\n",
            "Iteration 365, loss = 0.34581731\n",
            "Iteration 366, loss = 0.34541740\n",
            "Iteration 367, loss = 0.34501786\n",
            "Iteration 368, loss = 0.34461866\n",
            "Iteration 369, loss = 0.34421982\n",
            "Iteration 370, loss = 0.34382132\n",
            "Iteration 371, loss = 0.34342316\n",
            "Iteration 372, loss = 0.34302535\n",
            "Iteration 373, loss = 0.34262788\n",
            "Iteration 374, loss = 0.34223076\n",
            "Iteration 375, loss = 0.34183398\n",
            "Iteration 376, loss = 0.34143755\n",
            "Iteration 377, loss = 0.34104145\n",
            "Iteration 378, loss = 0.34064570\n",
            "Iteration 379, loss = 0.34025029\n",
            "Iteration 380, loss = 0.33985522\n",
            "Iteration 381, loss = 0.33946049\n",
            "Iteration 382, loss = 0.33906610\n",
            "Iteration 383, loss = 0.33867205\n",
            "Iteration 384, loss = 0.33827833\n",
            "Iteration 385, loss = 0.33788496\n",
            "Iteration 386, loss = 0.33749193\n",
            "Iteration 387, loss = 0.33709923\n",
            "Iteration 388, loss = 0.33670687\n",
            "Iteration 389, loss = 0.33631485\n",
            "Iteration 390, loss = 0.33592316\n",
            "Iteration 391, loss = 0.33553181\n",
            "Iteration 392, loss = 0.33514080\n",
            "Iteration 393, loss = 0.33475013\n",
            "Iteration 394, loss = 0.33435979\n",
            "Iteration 395, loss = 0.33396980\n",
            "Iteration 396, loss = 0.33358013\n",
            "Iteration 397, loss = 0.33319081\n",
            "Iteration 398, loss = 0.33280182\n",
            "Iteration 399, loss = 0.33241317\n",
            "Iteration 400, loss = 0.33202486\n",
            "Iteration 401, loss = 0.33163688\n",
            "Iteration 402, loss = 0.33124924\n",
            "Iteration 403, loss = 0.33086194\n",
            "Iteration 404, loss = 0.33047498\n",
            "Iteration 405, loss = 0.33008836\n",
            "Iteration 406, loss = 0.32970208\n",
            "Iteration 407, loss = 0.32931614\n",
            "Iteration 408, loss = 0.32893053\n",
            "Iteration 409, loss = 0.32854527\n",
            "Iteration 410, loss = 0.32816034\n",
            "Iteration 411, loss = 0.32777576\n",
            "Iteration 412, loss = 0.32739152\n",
            "Iteration 413, loss = 0.32700762\n",
            "Iteration 414, loss = 0.32662406\n",
            "Iteration 415, loss = 0.32624085\n",
            "Iteration 416, loss = 0.32585797\n",
            "Iteration 417, loss = 0.32547545\n",
            "Iteration 418, loss = 0.32509326\n",
            "Iteration 419, loss = 0.32471142\n",
            "Iteration 420, loss = 0.32432992\n",
            "Iteration 421, loss = 0.32394877\n",
            "Iteration 422, loss = 0.32356797\n",
            "Iteration 423, loss = 0.32318751\n",
            "Iteration 424, loss = 0.32280740\n",
            "Iteration 425, loss = 0.32242764\n",
            "Iteration 426, loss = 0.32204823\n",
            "Iteration 427, loss = 0.32166916\n",
            "Iteration 428, loss = 0.32129044\n",
            "Iteration 429, loss = 0.32091208\n",
            "Iteration 430, loss = 0.32053406\n",
            "Iteration 431, loss = 0.32015640\n",
            "Iteration 432, loss = 0.31977908\n",
            "Iteration 433, loss = 0.31940212\n",
            "Iteration 434, loss = 0.31902551\n",
            "Iteration 435, loss = 0.31864926\n",
            "Iteration 436, loss = 0.31827336\n",
            "Iteration 437, loss = 0.31789781\n",
            "Iteration 438, loss = 0.31752263\n",
            "Iteration 439, loss = 0.31714779\n",
            "Iteration 440, loss = 0.31677332\n",
            "Iteration 441, loss = 0.31639920\n",
            "Iteration 442, loss = 0.31602544\n",
            "Iteration 443, loss = 0.31565203\n",
            "Iteration 444, loss = 0.31527899\n",
            "Iteration 445, loss = 0.31490631\n",
            "Iteration 446, loss = 0.31453399\n",
            "Iteration 447, loss = 0.31416225\n",
            "Iteration 448, loss = 0.31379083\n",
            "Iteration 449, loss = 0.31341974\n",
            "Iteration 450, loss = 0.31304900\n",
            "Iteration 451, loss = 0.31267860\n",
            "Iteration 452, loss = 0.31230855\n",
            "Iteration 453, loss = 0.31193886\n",
            "Iteration 454, loss = 0.31156952\n",
            "Iteration 455, loss = 0.31120055\n",
            "Iteration 456, loss = 0.31083194\n",
            "Iteration 457, loss = 0.31046377\n",
            "Iteration 458, loss = 0.31009599\n",
            "Iteration 459, loss = 0.30972857\n",
            "Iteration 460, loss = 0.30936150\n",
            "Iteration 461, loss = 0.30899479\n",
            "Iteration 462, loss = 0.30862849\n",
            "Iteration 463, loss = 0.30826257\n",
            "Iteration 464, loss = 0.30789701\n",
            "Iteration 465, loss = 0.30753183\n",
            "Iteration 466, loss = 0.30716704\n",
            "Iteration 467, loss = 0.30680261\n",
            "Iteration 468, loss = 0.30643860\n",
            "Iteration 469, loss = 0.30607492\n",
            "Iteration 470, loss = 0.30571165\n",
            "Iteration 471, loss = 0.30534875\n",
            "Iteration 472, loss = 0.30498623\n",
            "Iteration 473, loss = 0.30462410\n",
            "Iteration 474, loss = 0.30426236\n",
            "Iteration 475, loss = 0.30390099\n",
            "Iteration 476, loss = 0.30354000\n",
            "Iteration 477, loss = 0.30317941\n",
            "Iteration 478, loss = 0.30281919\n",
            "Iteration 479, loss = 0.30245936\n",
            "Iteration 480, loss = 0.30209993\n",
            "Iteration 481, loss = 0.30174088\n",
            "Iteration 482, loss = 0.30138396\n",
            "Iteration 483, loss = 0.30103099\n",
            "Iteration 484, loss = 0.30067863\n",
            "Iteration 485, loss = 0.30032695\n",
            "Iteration 486, loss = 0.29997590\n",
            "Iteration 487, loss = 0.29962556\n",
            "Iteration 488, loss = 0.29927590\n",
            "Iteration 489, loss = 0.29892691\n",
            "Iteration 490, loss = 0.29857854\n",
            "Iteration 491, loss = 0.29823075\n",
            "Iteration 492, loss = 0.29788352\n",
            "Iteration 493, loss = 0.29753680\n",
            "Iteration 494, loss = 0.29719057\n",
            "Iteration 495, loss = 0.29684479\n",
            "Iteration 496, loss = 0.29649946\n",
            "Iteration 497, loss = 0.29615456\n",
            "Iteration 498, loss = 0.29581010\n",
            "Iteration 499, loss = 0.29546608\n",
            "Iteration 500, loss = 0.29512246\n",
            "Iteration 501, loss = 0.29477929\n",
            "Iteration 502, loss = 0.29443655\n",
            "Iteration 503, loss = 0.29409424\n",
            "Iteration 504, loss = 0.29375236\n",
            "Iteration 505, loss = 0.29341090\n",
            "Iteration 506, loss = 0.29306986\n",
            "Iteration 507, loss = 0.29272924\n",
            "Iteration 508, loss = 0.29238903\n",
            "Iteration 509, loss = 0.29204922\n",
            "Iteration 510, loss = 0.29170983\n",
            "Iteration 511, loss = 0.29137082\n",
            "Iteration 512, loss = 0.29103221\n",
            "Iteration 513, loss = 0.29069400\n",
            "Iteration 514, loss = 0.29035619\n",
            "Iteration 515, loss = 0.29001877\n",
            "Iteration 516, loss = 0.28968175\n",
            "Iteration 517, loss = 0.28934512\n",
            "Iteration 518, loss = 0.28900888\n",
            "Iteration 519, loss = 0.28867305\n",
            "Iteration 520, loss = 0.28833760\n",
            "Iteration 521, loss = 0.28800256\n",
            "Iteration 522, loss = 0.28766790\n",
            "Iteration 523, loss = 0.28733365\n",
            "Iteration 524, loss = 0.28699978\n",
            "Iteration 525, loss = 0.28666632\n",
            "Iteration 526, loss = 0.28633324\n",
            "Iteration 527, loss = 0.28600056\n",
            "Iteration 528, loss = 0.28566828\n",
            "Iteration 529, loss = 0.28533639\n",
            "Iteration 530, loss = 0.28500490\n",
            "Iteration 531, loss = 0.28467380\n",
            "Iteration 532, loss = 0.28434309\n",
            "Iteration 533, loss = 0.28401279\n",
            "Iteration 534, loss = 0.28368288\n",
            "Iteration 535, loss = 0.28335337\n",
            "Iteration 536, loss = 0.28302425\n",
            "Iteration 537, loss = 0.28269553\n",
            "Iteration 538, loss = 0.28236721\n",
            "Iteration 539, loss = 0.28203929\n",
            "Iteration 540, loss = 0.28171177\n",
            "Iteration 541, loss = 0.28138465\n",
            "Iteration 542, loss = 0.28105792\n",
            "Iteration 543, loss = 0.28073160\n",
            "Iteration 544, loss = 0.28040568\n",
            "Iteration 545, loss = 0.28008015\n",
            "Iteration 546, loss = 0.27975503\n",
            "Iteration 547, loss = 0.27943031\n",
            "Iteration 548, loss = 0.27910600\n",
            "Iteration 549, loss = 0.27878208\n",
            "Iteration 550, loss = 0.27845857\n",
            "Iteration 551, loss = 0.27813546\n",
            "Iteration 552, loss = 0.27781276\n",
            "Iteration 553, loss = 0.27749046\n",
            "Iteration 554, loss = 0.27716857\n",
            "Iteration 555, loss = 0.27684708\n",
            "Iteration 556, loss = 0.27652600\n",
            "Iteration 557, loss = 0.27620532\n",
            "Iteration 558, loss = 0.27588505\n",
            "Iteration 559, loss = 0.27556519\n",
            "Iteration 560, loss = 0.27524573\n",
            "Iteration 561, loss = 0.27492668\n",
            "Iteration 562, loss = 0.27460804\n",
            "Iteration 563, loss = 0.27428981\n",
            "Iteration 564, loss = 0.27397199\n",
            "Iteration 565, loss = 0.27365458\n",
            "Iteration 566, loss = 0.27333758\n",
            "Iteration 567, loss = 0.27302099\n",
            "Iteration 568, loss = 0.27270481\n",
            "Iteration 569, loss = 0.27238904\n",
            "Iteration 570, loss = 0.27207369\n",
            "Iteration 571, loss = 0.27175874\n",
            "Iteration 572, loss = 0.27144421\n",
            "Iteration 573, loss = 0.27113009\n",
            "Iteration 574, loss = 0.27081639\n",
            "Iteration 575, loss = 0.27050310\n",
            "Iteration 576, loss = 0.27019022\n",
            "Iteration 577, loss = 0.26987775\n",
            "Iteration 578, loss = 0.26956571\n",
            "Iteration 579, loss = 0.26925407\n",
            "Iteration 580, loss = 0.26894286\n",
            "Iteration 581, loss = 0.26863205\n",
            "Iteration 582, loss = 0.26832167\n",
            "Iteration 583, loss = 0.26801170\n",
            "Iteration 584, loss = 0.26770215\n",
            "Iteration 585, loss = 0.26739301\n",
            "Iteration 586, loss = 0.26708430\n",
            "Iteration 587, loss = 0.26677600\n",
            "Iteration 588, loss = 0.26646812\n",
            "Iteration 589, loss = 0.26616066\n",
            "Iteration 590, loss = 0.26585362\n",
            "Iteration 591, loss = 0.26554699\n",
            "Iteration 592, loss = 0.26524079\n",
            "Iteration 593, loss = 0.26493501\n",
            "Iteration 594, loss = 0.26462964\n",
            "Iteration 595, loss = 0.26432470\n",
            "Iteration 596, loss = 0.26402018\n",
            "Iteration 597, loss = 0.26371608\n",
            "Iteration 598, loss = 0.26341240\n",
            "Iteration 599, loss = 0.26310914\n",
            "Iteration 600, loss = 0.26280631\n",
            "Iteration 601, loss = 0.26250390\n",
            "Iteration 602, loss = 0.26220191\n",
            "Iteration 603, loss = 0.26190034\n",
            "Iteration 604, loss = 0.26159920\n",
            "Iteration 605, loss = 0.26129848\n",
            "Iteration 606, loss = 0.26099818\n",
            "Iteration 607, loss = 0.26069831\n",
            "Iteration 608, loss = 0.26039886\n",
            "Iteration 609, loss = 0.26009983\n",
            "Iteration 610, loss = 0.25980123\n",
            "Iteration 611, loss = 0.25950306\n",
            "Iteration 612, loss = 0.25920531\n",
            "Iteration 613, loss = 0.25890799\n",
            "Iteration 614, loss = 0.25861109\n",
            "Iteration 615, loss = 0.25831461\n",
            "Iteration 616, loss = 0.25801857\n",
            "Iteration 617, loss = 0.25772295\n",
            "Iteration 618, loss = 0.25742775\n",
            "Iteration 619, loss = 0.25713298\n",
            "Iteration 620, loss = 0.25683864\n",
            "Iteration 621, loss = 0.25654473\n",
            "Iteration 622, loss = 0.25625124\n",
            "Iteration 623, loss = 0.25595818\n",
            "Iteration 624, loss = 0.25566555\n",
            "Iteration 625, loss = 0.25537335\n",
            "Iteration 626, loss = 0.25508157\n",
            "Iteration 627, loss = 0.25479490\n",
            "Iteration 628, loss = 0.25450884\n",
            "Iteration 629, loss = 0.25422335\n",
            "Iteration 630, loss = 0.25393853\n",
            "Iteration 631, loss = 0.25365443\n",
            "Iteration 632, loss = 0.25337111\n",
            "Iteration 633, loss = 0.25308855\n",
            "Iteration 634, loss = 0.25280674\n",
            "Iteration 635, loss = 0.25252562\n",
            "Iteration 636, loss = 0.25224514\n",
            "Iteration 637, loss = 0.25196524\n",
            "Iteration 638, loss = 0.25168585\n",
            "Iteration 639, loss = 0.25140695\n",
            "Iteration 640, loss = 0.25112851\n",
            "Iteration 641, loss = 0.25085051\n",
            "Iteration 642, loss = 0.25057295\n",
            "Iteration 643, loss = 0.25029583\n",
            "Iteration 644, loss = 0.25001915\n",
            "Iteration 645, loss = 0.24974293\n",
            "Iteration 646, loss = 0.24946717\n",
            "Iteration 647, loss = 0.24919188\n",
            "Iteration 648, loss = 0.24891704\n",
            "Iteration 649, loss = 0.24864266\n",
            "Iteration 650, loss = 0.24836873\n",
            "Iteration 651, loss = 0.24809524\n",
            "Iteration 652, loss = 0.24782219\n",
            "Iteration 653, loss = 0.24754957\n",
            "Iteration 654, loss = 0.24727737\n",
            "Iteration 655, loss = 0.24700559\n",
            "Iteration 656, loss = 0.24673423\n",
            "Iteration 657, loss = 0.24646328\n",
            "Iteration 658, loss = 0.24619274\n",
            "Iteration 659, loss = 0.24592262\n",
            "Iteration 660, loss = 0.24565291\n",
            "Iteration 661, loss = 0.24538362\n",
            "Iteration 662, loss = 0.24511474\n",
            "Iteration 663, loss = 0.24484627\n",
            "Iteration 664, loss = 0.24457822\n",
            "Iteration 665, loss = 0.24431058\n",
            "Iteration 666, loss = 0.24404335\n",
            "Iteration 667, loss = 0.24377652\n",
            "Iteration 668, loss = 0.24351011\n",
            "Iteration 669, loss = 0.24324410\n",
            "Iteration 670, loss = 0.24297850\n",
            "Iteration 671, loss = 0.24271331\n",
            "Iteration 672, loss = 0.24244853\n",
            "Iteration 673, loss = 0.24218414\n",
            "Iteration 674, loss = 0.24192017\n",
            "Iteration 675, loss = 0.24165661\n",
            "Iteration 676, loss = 0.24139344\n",
            "Iteration 677, loss = 0.24113069\n",
            "Iteration 678, loss = 0.24086833\n",
            "Iteration 679, loss = 0.24060639\n",
            "Iteration 680, loss = 0.24034484\n",
            "Iteration 681, loss = 0.24008371\n",
            "Iteration 682, loss = 0.23982297\n",
            "Iteration 683, loss = 0.23956265\n",
            "Iteration 684, loss = 0.23930272\n",
            "Iteration 685, loss = 0.23904319\n",
            "Iteration 686, loss = 0.23878407\n",
            "Iteration 687, loss = 0.23852536\n",
            "Iteration 688, loss = 0.23826704\n",
            "Iteration 689, loss = 0.23800913\n",
            "Iteration 690, loss = 0.23775162\n",
            "Iteration 691, loss = 0.23749452\n",
            "Iteration 692, loss = 0.23723781\n",
            "Iteration 693, loss = 0.23698151\n",
            "Iteration 694, loss = 0.23672561\n",
            "Iteration 695, loss = 0.23647011\n",
            "Iteration 696, loss = 0.23621501\n",
            "Iteration 697, loss = 0.23596032\n",
            "Iteration 698, loss = 0.23570602\n",
            "Iteration 699, loss = 0.23545213\n",
            "Iteration 700, loss = 0.23519864\n",
            "Iteration 701, loss = 0.23494554\n",
            "Iteration 702, loss = 0.23469286\n",
            "Iteration 703, loss = 0.23444056\n",
            "Iteration 704, loss = 0.23418867\n",
            "Iteration 705, loss = 0.23393719\n",
            "Iteration 706, loss = 0.23368609\n",
            "Iteration 707, loss = 0.23343540\n",
            "Iteration 708, loss = 0.23318512\n",
            "Iteration 709, loss = 0.23293546\n",
            "Iteration 710, loss = 0.23268820\n",
            "Iteration 711, loss = 0.23244134\n",
            "Iteration 712, loss = 0.23219498\n",
            "Iteration 713, loss = 0.23194919\n",
            "Iteration 714, loss = 0.23170400\n",
            "Iteration 715, loss = 0.23145943\n",
            "Iteration 716, loss = 0.23121546\n",
            "Iteration 717, loss = 0.23097203\n",
            "Iteration 718, loss = 0.23072913\n",
            "Iteration 719, loss = 0.23048670\n",
            "Iteration 720, loss = 0.23024470\n",
            "Iteration 721, loss = 0.23000311\n",
            "Iteration 722, loss = 0.22976193\n",
            "Iteration 723, loss = 0.22952317\n",
            "Iteration 724, loss = 0.22928487\n",
            "Iteration 725, loss = 0.22904706\n",
            "Iteration 726, loss = 0.22880980\n",
            "Iteration 727, loss = 0.22857309\n",
            "Iteration 728, loss = 0.22833691\n",
            "Iteration 729, loss = 0.22810126\n",
            "Iteration 730, loss = 0.22786607\n",
            "Iteration 731, loss = 0.22763133\n",
            "Iteration 732, loss = 0.22739702\n",
            "Iteration 733, loss = 0.22716309\n",
            "Iteration 734, loss = 0.22692955\n",
            "Iteration 735, loss = 0.22669637\n",
            "Iteration 736, loss = 0.22646358\n",
            "Iteration 737, loss = 0.22623116\n",
            "Iteration 738, loss = 0.22599913\n",
            "Iteration 739, loss = 0.22576749\n",
            "Iteration 740, loss = 0.22553626\n",
            "Iteration 741, loss = 0.22530542\n",
            "Iteration 742, loss = 0.22507500\n",
            "Iteration 743, loss = 0.22484497\n",
            "Iteration 744, loss = 0.22461533\n",
            "Iteration 745, loss = 0.22438610\n",
            "Iteration 746, loss = 0.22415725\n",
            "Iteration 747, loss = 0.22392878\n",
            "Iteration 748, loss = 0.22370071\n",
            "Iteration 749, loss = 0.22347301\n",
            "Iteration 750, loss = 0.22324569\n",
            "Iteration 751, loss = 0.22301874\n",
            "Iteration 752, loss = 0.22279218\n",
            "Iteration 753, loss = 0.22256600\n",
            "Iteration 754, loss = 0.22234020\n",
            "Iteration 755, loss = 0.22211477\n",
            "Iteration 756, loss = 0.22188972\n",
            "Iteration 757, loss = 0.22166505\n",
            "Iteration 758, loss = 0.22144076\n",
            "Iteration 759, loss = 0.22121685\n",
            "Iteration 760, loss = 0.22099331\n",
            "Iteration 761, loss = 0.22077014\n",
            "Iteration 762, loss = 0.22054735\n",
            "Iteration 763, loss = 0.22032493\n",
            "Iteration 764, loss = 0.22010289\n",
            "Iteration 765, loss = 0.21988121\n",
            "Iteration 766, loss = 0.21965991\n",
            "Iteration 767, loss = 0.21943898\n",
            "Iteration 768, loss = 0.21921841\n",
            "Iteration 769, loss = 0.21899822\n",
            "Iteration 770, loss = 0.21877840\n",
            "Iteration 771, loss = 0.21855894\n",
            "Iteration 772, loss = 0.21833985\n",
            "Iteration 773, loss = 0.21812113\n",
            "Iteration 774, loss = 0.21790278\n",
            "Iteration 775, loss = 0.21768479\n",
            "Iteration 776, loss = 0.21746717\n",
            "Iteration 777, loss = 0.21724991\n",
            "Iteration 778, loss = 0.21703302\n",
            "Iteration 779, loss = 0.21681649\n",
            "Iteration 780, loss = 0.21660033\n",
            "Iteration 781, loss = 0.21638452\n",
            "Iteration 782, loss = 0.21616908\n",
            "Iteration 783, loss = 0.21595400\n",
            "Iteration 784, loss = 0.21573928\n",
            "Iteration 785, loss = 0.21552493\n",
            "Iteration 786, loss = 0.21531093\n",
            "Iteration 787, loss = 0.21509729\n",
            "Iteration 788, loss = 0.21488401\n",
            "Iteration 789, loss = 0.21467109\n",
            "Iteration 790, loss = 0.21445853\n",
            "Iteration 791, loss = 0.21424633\n",
            "Iteration 792, loss = 0.21403448\n",
            "Iteration 793, loss = 0.21382299\n",
            "Iteration 794, loss = 0.21361185\n",
            "Iteration 795, loss = 0.21340107\n",
            "Iteration 796, loss = 0.21319065\n",
            "Iteration 797, loss = 0.21298058\n",
            "Iteration 798, loss = 0.21277086\n",
            "Iteration 799, loss = 0.21256149\n",
            "Iteration 800, loss = 0.21235248\n",
            "Iteration 801, loss = 0.21214382\n",
            "Iteration 802, loss = 0.21193552\n",
            "Iteration 803, loss = 0.21172756\n",
            "Iteration 804, loss = 0.21151996\n",
            "Iteration 805, loss = 0.21131270\n",
            "Iteration 806, loss = 0.21110580\n",
            "Iteration 807, loss = 0.21089924\n",
            "Iteration 808, loss = 0.21069304\n",
            "Iteration 809, loss = 0.21048718\n",
            "Iteration 810, loss = 0.21028167\n",
            "Iteration 811, loss = 0.21007651\n",
            "Iteration 812, loss = 0.20987170\n",
            "Iteration 813, loss = 0.20966723\n",
            "Iteration 814, loss = 0.20946311\n",
            "Iteration 815, loss = 0.20925933\n",
            "Iteration 816, loss = 0.20905590\n",
            "Iteration 817, loss = 0.20885281\n",
            "Iteration 818, loss = 0.20865007\n",
            "Iteration 819, loss = 0.20844767\n",
            "Iteration 820, loss = 0.20824562\n",
            "Iteration 821, loss = 0.20804391\n",
            "Iteration 822, loss = 0.20784254\n",
            "Iteration 823, loss = 0.20764151\n",
            "Iteration 824, loss = 0.20744082\n",
            "Iteration 825, loss = 0.20724048\n",
            "Iteration 826, loss = 0.20704048\n",
            "Iteration 827, loss = 0.20684081\n",
            "Iteration 828, loss = 0.20664149\n",
            "Iteration 829, loss = 0.20644250\n",
            "Iteration 830, loss = 0.20624386\n",
            "Iteration 831, loss = 0.20604555\n",
            "Iteration 832, loss = 0.20584758\n",
            "Iteration 833, loss = 0.20564995\n",
            "Iteration 834, loss = 0.20545265\n",
            "Iteration 835, loss = 0.20525569\n",
            "Iteration 836, loss = 0.20505907\n",
            "Iteration 837, loss = 0.20486278\n",
            "Iteration 838, loss = 0.20466683\n",
            "Iteration 839, loss = 0.20447121\n",
            "Iteration 840, loss = 0.20427592\n",
            "Iteration 841, loss = 0.20408098\n",
            "Iteration 842, loss = 0.20388636\n",
            "Iteration 843, loss = 0.20369207\n",
            "Iteration 844, loss = 0.20349812\n",
            "Iteration 845, loss = 0.20330450\n",
            "Iteration 846, loss = 0.20311122\n",
            "Iteration 847, loss = 0.20291826\n",
            "Iteration 848, loss = 0.20272563\n",
            "Iteration 849, loss = 0.20253334\n",
            "Iteration 850, loss = 0.20234137\n",
            "Iteration 851, loss = 0.20214973\n",
            "Iteration 852, loss = 0.20195842\n",
            "Iteration 853, loss = 0.20176744\n",
            "Iteration 854, loss = 0.20157679\n",
            "Iteration 855, loss = 0.20138647\n",
            "Iteration 856, loss = 0.20119647\n",
            "Iteration 857, loss = 0.20100680\n",
            "Iteration 858, loss = 0.20081745\n",
            "Iteration 859, loss = 0.20062843\n",
            "Iteration 860, loss = 0.20043974\n",
            "Iteration 861, loss = 0.20025137\n",
            "Iteration 862, loss = 0.20006332\n",
            "Iteration 863, loss = 0.19987560\n",
            "Iteration 864, loss = 0.19968820\n",
            "Iteration 865, loss = 0.19950112\n",
            "Iteration 866, loss = 0.19931437\n",
            "Iteration 867, loss = 0.19912794\n",
            "Iteration 868, loss = 0.19894183\n",
            "Iteration 869, loss = 0.19875604\n",
            "Iteration 870, loss = 0.19857057\n",
            "Iteration 871, loss = 0.19838542\n",
            "Iteration 872, loss = 0.19820059\n",
            "Iteration 873, loss = 0.19801608\n",
            "Iteration 874, loss = 0.19783189\n",
            "Iteration 875, loss = 0.19764801\n",
            "Iteration 876, loss = 0.19746446\n",
            "Iteration 877, loss = 0.19728122\n",
            "Iteration 878, loss = 0.19709830\n",
            "Iteration 879, loss = 0.19691570\n",
            "Iteration 880, loss = 0.19673341\n",
            "Iteration 881, loss = 0.19655143\n",
            "Iteration 882, loss = 0.19636978\n",
            "Iteration 883, loss = 0.19618843\n",
            "Iteration 884, loss = 0.19600740\n",
            "Iteration 885, loss = 0.19582669\n",
            "Iteration 886, loss = 0.19564628\n",
            "Iteration 887, loss = 0.19546619\n",
            "Iteration 888, loss = 0.19528642\n",
            "Iteration 889, loss = 0.19510695\n",
            "Iteration 890, loss = 0.19492780\n",
            "Iteration 891, loss = 0.19474895\n",
            "Iteration 892, loss = 0.19457042\n",
            "Iteration 893, loss = 0.19439220\n",
            "Iteration 894, loss = 0.19421428\n",
            "Iteration 895, loss = 0.19403668\n",
            "Iteration 896, loss = 0.19385938\n",
            "Iteration 897, loss = 0.19368240\n",
            "Iteration 898, loss = 0.19350572\n",
            "Iteration 899, loss = 0.19332934\n",
            "Iteration 900, loss = 0.19315328\n",
            "Iteration 901, loss = 0.19297752\n",
            "Iteration 902, loss = 0.19280206\n",
            "Iteration 903, loss = 0.19262691\n",
            "Iteration 904, loss = 0.19245207\n",
            "Iteration 905, loss = 0.19227753\n",
            "Iteration 906, loss = 0.19210330\n",
            "Iteration 907, loss = 0.19192936\n",
            "Iteration 908, loss = 0.19175616\n",
            "Iteration 909, loss = 0.19158324\n",
            "Iteration 910, loss = 0.19141056\n",
            "Iteration 911, loss = 0.19123820\n",
            "Iteration 912, loss = 0.19106622\n",
            "Iteration 913, loss = 0.19089465\n",
            "Iteration 914, loss = 0.19072349\n",
            "Iteration 915, loss = 0.19055281\n",
            "Iteration 916, loss = 0.19038233\n",
            "Iteration 917, loss = 0.19021225\n",
            "Iteration 918, loss = 0.19004249\n",
            "Iteration 919, loss = 0.18987305\n",
            "Iteration 920, loss = 0.18970390\n",
            "Iteration 921, loss = 0.18953505\n",
            "Iteration 922, loss = 0.18936649\n",
            "Iteration 923, loss = 0.18919823\n",
            "Iteration 924, loss = 0.18903026\n",
            "Iteration 925, loss = 0.18886259\n",
            "Iteration 926, loss = 0.18869521\n",
            "Iteration 927, loss = 0.18852813\n",
            "Iteration 928, loss = 0.18836136\n",
            "Iteration 929, loss = 0.18819489\n",
            "Iteration 930, loss = 0.18802871\n",
            "Iteration 931, loss = 0.18786284\n",
            "Iteration 932, loss = 0.18769727\n",
            "Iteration 933, loss = 0.18753199\n",
            "Iteration 934, loss = 0.18736701\n",
            "Iteration 935, loss = 0.18720232\n",
            "Iteration 936, loss = 0.18703792\n",
            "Iteration 937, loss = 0.18687381\n",
            "Iteration 938, loss = 0.18671000\n",
            "Iteration 939, loss = 0.18654647\n",
            "Iteration 940, loss = 0.18638324\n",
            "Iteration 941, loss = 0.18622030\n",
            "Iteration 942, loss = 0.18605766\n",
            "Iteration 943, loss = 0.18589537\n",
            "Iteration 944, loss = 0.18573337\n",
            "Iteration 945, loss = 0.18557166\n",
            "Iteration 946, loss = 0.18541023\n",
            "Iteration 947, loss = 0.18524910\n",
            "Iteration 948, loss = 0.18508825\n",
            "Iteration 949, loss = 0.18492768\n",
            "Iteration 950, loss = 0.18476740\n",
            "Iteration 951, loss = 0.18460741\n",
            "Iteration 952, loss = 0.18444771\n",
            "Iteration 953, loss = 0.18428829\n",
            "Iteration 954, loss = 0.18412915\n",
            "Iteration 955, loss = 0.18397030\n",
            "Iteration 956, loss = 0.18381173\n",
            "Iteration 957, loss = 0.18365345\n",
            "Iteration 958, loss = 0.18349545\n",
            "Iteration 959, loss = 0.18333773\n",
            "Iteration 960, loss = 0.18318030\n",
            "Iteration 961, loss = 0.18302314\n",
            "Iteration 962, loss = 0.18286627\n",
            "Iteration 963, loss = 0.18270968\n",
            "Iteration 964, loss = 0.18255337\n",
            "Iteration 965, loss = 0.18239733\n",
            "Iteration 966, loss = 0.18224158\n",
            "Iteration 967, loss = 0.18208610\n",
            "Iteration 968, loss = 0.18193091\n",
            "Iteration 969, loss = 0.18177598\n",
            "Iteration 970, loss = 0.18162134\n",
            "Iteration 971, loss = 0.18146697\n",
            "Iteration 972, loss = 0.18131288\n",
            "Iteration 973, loss = 0.18115906\n",
            "Iteration 974, loss = 0.18100551\n",
            "Iteration 975, loss = 0.18085224\n",
            "Iteration 976, loss = 0.18069924\n",
            "Iteration 977, loss = 0.18054651\n",
            "Iteration 978, loss = 0.18039406\n",
            "Iteration 979, loss = 0.18024187\n",
            "Iteration 980, loss = 0.18008996\n",
            "Iteration 981, loss = 0.17993831\n",
            "Iteration 982, loss = 0.17978694\n",
            "Iteration 983, loss = 0.17963583\n",
            "Iteration 984, loss = 0.17948499\n",
            "Iteration 985, loss = 0.17933442\n",
            "Iteration 986, loss = 0.17918412\n",
            "Iteration 987, loss = 0.17903408\n",
            "Iteration 988, loss = 0.17888431\n",
            "Iteration 989, loss = 0.17873481\n",
            "Iteration 990, loss = 0.17858557\n",
            "Iteration 991, loss = 0.17843659\n",
            "Iteration 992, loss = 0.17828788\n",
            "Iteration 993, loss = 0.17813943\n",
            "Iteration 994, loss = 0.17799124\n",
            "Iteration 995, loss = 0.17784332\n",
            "Iteration 996, loss = 0.17769566\n",
            "Iteration 997, loss = 0.17754825\n",
            "Iteration 998, loss = 0.17740111\n",
            "Iteration 999, loss = 0.17725423\n",
            "Iteration 1000, loss = 0.17710761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=0.05, hidden_layer_sizes=3, max_iter=1000, solver='sgd',\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.05, hidden_layer_sizes=3, max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.05, hidden_layer_sizes=3, max_iter=1000, solver=&#x27;sgd&#x27;,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# avaliação\n",
        "y_pred = clf.predict(X_test)\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlBjZTXy8GY-",
        "outputId": "3eafcb6b-79d7-43cd-af0f-67ed881634f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        16\n",
            "           1       0.88      0.93      0.90        15\n",
            "           2       0.92      0.86      0.89        14\n",
            "\n",
            "    accuracy                           0.93        45\n",
            "   macro avg       0.93      0.93      0.93        45\n",
            "weighted avg       0.93      0.93      0.93        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gráfico da curva de aprendizado\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('epocas')\n",
        "plt.ylabel('função de custo de erro')\n",
        "plt.plot(clf.loss_curve_)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QDx77lzm8N5V",
        "outputId": "48a105b5-aa61-4b3c-b15b-51f5ec4fcbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_xC3nGn9Fxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}